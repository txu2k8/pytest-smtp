<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8"/>
    <title>Test Report</title>
    <style>body {
  font-family: Helvetica, Arial, sans-serif;
  font-size: 12px;
  /* do not increase min-width as some may use split screens */
  min-width: 800px;
  color: #999;
}

h1 {
  font-size: 24px;
  color: black;
}

h2 {
  font-size: 16px;
  color: black;
}

p {
  color: black;
}

a {
  color: #999;
}

table {
  border-collapse: collapse;
}

/******************************
 * SUMMARY INFORMATION
 ******************************/
#environment td {
  padding: 5px;
  border: 1px solid #E6E6E6;
}
#environment tr:nth-child(odd) {
  background-color: #f6f6f6;
}

/******************************
 * TEST RESULT COLORS
 ******************************/
span.passed,
.passed .col-result {
  color: green;
}

span.skipped,
span.xfailed,
span.rerun,
.skipped .col-result,
.xfailed .col-result,
.rerun .col-result {
  color: orange;
}

span.error,
span.failed,
span.xpassed,
.error .col-result,
.failed .col-result,
.xpassed .col-result {
  color: red;
}

/******************************
 * RESULTS TABLE
 *
 * 1. Table Layout
 * 2. Extra
 * 3. Sorting items
 *
 ******************************/
/*------------------
 * 1. Table Layout
 *------------------*/
#results-table {
  border: 1px solid #e6e6e6;
  color: #999;
  font-size: 12px;
  width: 100%;
}
#results-table th,
#results-table td {
  padding: 5px;
  border: 1px solid #E6E6E6;
  text-align: left;
}
#results-table th {
  font-weight: bold;
}

/*------------------
 * 2. Extra
 *------------------*/
.log {
  background-color: #e6e6e6;
  border: 1px solid #e6e6e6;
  color: black;
  display: block;
  font-family: "Courier New", Courier, monospace;
  height: 230px;
  overflow-y: scroll;
  padding: 5px;
  white-space: pre-wrap;
}
.log:only-child {
  height: inherit;
}

div.image {
  border: 1px solid #e6e6e6;
  float: right;
  height: 240px;
  margin-left: 5px;
  overflow: hidden;
  width: 320px;
}
div.image img {
  width: 320px;
}

div.video {
  border: 1px solid #e6e6e6;
  float: right;
  height: 240px;
  margin-left: 5px;
  overflow: hidden;
  width: 320px;
}
div.video video {
  overflow: hidden;
  width: 320px;
  height: 240px;
}

.collapsed {
  display: none;
}

.expander::after {
  content: " (show details)";
  color: #BBB;
  font-style: italic;
  cursor: pointer;
}

.collapser::after {
  content: " (hide details)";
  color: #BBB;
  font-style: italic;
  cursor: pointer;
}

/*------------------
 * 3. Sorting items
 *------------------*/
.sortable {
  cursor: pointer;
}

.sort-icon {
  font-size: 0px;
  float: left;
  margin-right: 5px;
  margin-top: 5px;
  /*triangle*/
  width: 0;
  height: 0;
  border-left: 8px solid transparent;
  border-right: 8px solid transparent;
}
.inactive .sort-icon {
  /*finish triangle*/
  border-top: 8px solid #E6E6E6;
}
.asc.active .sort-icon {
  /*finish triangle*/
  border-bottom: 8px solid #999;
}
.desc.active .sort-icon {
  /*finish triangle*/
  border-top: 8px solid #999;
}
</style></head>
  <body onLoad="init()">
    <script>/* This Source Code Form is subject to the terms of the Mozilla Public
 * License, v. 2.0. If a copy of the MPL was not distributed with this file,
 * You can obtain one at http://mozilla.org/MPL/2.0/. */


function toArray(iter) {
    if (iter === null) {
        return null;
    }
    return Array.prototype.slice.call(iter);
}

function find(selector, elem) { // eslint-disable-line no-redeclare
    if (!elem) {
        elem = document;
    }
    return elem.querySelector(selector);
}

function findAll(selector, elem) {
    if (!elem) {
        elem = document;
    }
    return toArray(elem.querySelectorAll(selector));
}

function sortColumn(elem) {
    toggleSortStates(elem);
    const colIndex = toArray(elem.parentNode.childNodes).indexOf(elem);
    let key;
    if (elem.classList.contains('result')) {
        key = keyResult;
    } else if (elem.classList.contains('links')) {
        key = keyLink;
    } else {
        key = keyAlpha;
    }
    sortTable(elem, key(colIndex));
}

function showAllExtras() { // eslint-disable-line no-unused-vars
    findAll('.col-result').forEach(showExtras);
}

function hideAllExtras() { // eslint-disable-line no-unused-vars
    findAll('.col-result').forEach(hideExtras);
}

function showExtras(colresultElem) {
    const extras = colresultElem.parentNode.nextElementSibling;
    const expandcollapse = colresultElem.firstElementChild;
    extras.classList.remove('collapsed');
    expandcollapse.classList.remove('expander');
    expandcollapse.classList.add('collapser');
}

function hideExtras(colresultElem) {
    const extras = colresultElem.parentNode.nextElementSibling;
    const expandcollapse = colresultElem.firstElementChild;
    extras.classList.add('collapsed');
    expandcollapse.classList.remove('collapser');
    expandcollapse.classList.add('expander');
}

function showFilters() {
    const filterItems = document.getElementsByClassName('filter');
    for (let i = 0; i < filterItems.length; i++)
        filterItems[i].hidden = false;
}

function addCollapse() {
    // Add links for show/hide all
    const resulttable = find('table#results-table');
    const showhideall = document.createElement('p');
    showhideall.innerHTML = '<a href="javascript:showAllExtras()">Show all details</a> / ' +
                            '<a href="javascript:hideAllExtras()">Hide all details</a>';
    resulttable.parentElement.insertBefore(showhideall, resulttable);

    // Add show/hide link to each result
    findAll('.col-result').forEach(function(elem) {
        const collapsed = getQueryParameter('collapsed') || 'Passed';
        const extras = elem.parentNode.nextElementSibling;
        const expandcollapse = document.createElement('span');
        if (extras.classList.contains('collapsed')) {
            expandcollapse.classList.add('expander');
        } else if (collapsed.includes(elem.innerHTML)) {
            extras.classList.add('collapsed');
            expandcollapse.classList.add('expander');
        } else {
            expandcollapse.classList.add('collapser');
        }
        elem.appendChild(expandcollapse);

        elem.addEventListener('click', function(event) {
            if (event.currentTarget.parentNode.nextElementSibling.classList.contains('collapsed')) {
                showExtras(event.currentTarget);
            } else {
                hideExtras(event.currentTarget);
            }
        });
    });
}

function getQueryParameter(name) {
    const match = RegExp('[?&]' + name + '=([^&]*)').exec(window.location.search);
    return match && decodeURIComponent(match[1].replace(/\+/g, ' '));
}

function init () { // eslint-disable-line no-unused-vars
    resetSortHeaders();

    addCollapse();

    showFilters();

    sortColumn(find('.initial-sort'));

    findAll('.sortable').forEach(function(elem) {
        elem.addEventListener('click',
            function() {
                sortColumn(elem);
            }, false);
    });
}

function sortTable(clicked, keyFunc) {
    const rows = findAll('.results-table-row');
    const reversed = !clicked.classList.contains('asc');
    const sortedRows = sort(rows, keyFunc, reversed);
    /* Whole table is removed here because browsers acts much slower
     * when appending existing elements.
     */
    const thead = document.getElementById('results-table-head');
    document.getElementById('results-table').remove();
    const parent = document.createElement('table');
    parent.id = 'results-table';
    parent.appendChild(thead);
    sortedRows.forEach(function(elem) {
        parent.appendChild(elem);
    });
    document.getElementsByTagName('BODY')[0].appendChild(parent);
}

function sort(items, keyFunc, reversed) {
    const sortArray = items.map(function(item, i) {
        return [keyFunc(item), i];
    });

    sortArray.sort(function(a, b) {
        const keyA = a[0];
        const keyB = b[0];

        if (keyA == keyB) return 0;

        if (reversed) {
            return keyA < keyB ? 1 : -1;
        } else {
            return keyA > keyB ? 1 : -1;
        }
    });

    return sortArray.map(function(item) {
        const index = item[1];
        return items[index];
    });
}

function keyAlpha(colIndex) {
    return function(elem) {
        return elem.childNodes[1].childNodes[colIndex].firstChild.data.toLowerCase();
    };
}

function keyLink(colIndex) {
    return function(elem) {
        const dataCell = elem.childNodes[1].childNodes[colIndex].firstChild;
        return dataCell == null ? '' : dataCell.innerText.toLowerCase();
    };
}

function keyResult(colIndex) {
    return function(elem) {
        const strings = ['Error', 'Failed', 'Rerun', 'XFailed', 'XPassed',
            'Skipped', 'Passed'];
        return strings.indexOf(elem.childNodes[1].childNodes[colIndex].firstChild.data);
    };
}

function resetSortHeaders() {
    findAll('.sort-icon').forEach(function(elem) {
        elem.parentNode.removeChild(elem);
    });
    findAll('.sortable').forEach(function(elem) {
        const icon = document.createElement('div');
        icon.className = 'sort-icon';
        icon.textContent = 'vvv';
        elem.insertBefore(icon, elem.firstChild);
        elem.classList.remove('desc', 'active');
        elem.classList.add('asc', 'inactive');
    });
}

function toggleSortStates(elem) {
    //if active, toggle between asc and desc
    if (elem.classList.contains('active')) {
        elem.classList.toggle('asc');
        elem.classList.toggle('desc');
    }

    //if inactive, reset all other functions and add ascending active
    if (elem.classList.contains('inactive')) {
        resetSortHeaders();
        elem.classList.remove('inactive');
        elem.classList.add('active');
    }
}

function isAllRowsHidden(value) {
    return value.hidden == false;
}

function filterTable(elem) { // eslint-disable-line no-unused-vars
    const outcomeAtt = 'data-test-result';
    const outcome = elem.getAttribute(outcomeAtt);
    const classOutcome = outcome + ' results-table-row';
    const outcomeRows = document.getElementsByClassName(classOutcome);

    for(let i = 0; i < outcomeRows.length; i++){
        outcomeRows[i].hidden = !elem.checked;
    }

    const rows = findAll('.results-table-row').filter(isAllRowsHidden);
    const allRowsHidden = rows.length == 0 ? true : false;
    const notFoundMessage = document.getElementById('not-found-message');
    notFoundMessage.hidden = !allRowsHidden;
}
</script>
    <h1>pytest_report.html</h1>
    <p>Report generated on 31-Mar-2021 at 15:03:24 by <a href="https://pypi.python.org/pypi/pytest-html">pytest-html</a> v3.1.1</p>
    <h2>Environment</h2>
    <table id="environment">
      <tr>
        <td>Packages</td>
        <td>{"pluggy": "0.13.1", "py": "1.10.0", "pytest": "6.2.2"}</td></tr>
      <tr>
        <td>Platform</td>
        <td>Windows-10-10.0.19041-SP0</td></tr>
      <tr>
        <td>Plugins</td>
        <td>{"html": "3.1.1", "metadata": "1.11.0", "repeat": "0.9.1", "smtp": "0.1"}</td></tr>
      <tr>
        <td>Python</td>
        <td>3.9.0</td></tr></table>
    <h2>Summary</h2>
    <p>3 tests ran in 0.45 seconds. </p>
    <p class="filter" hidden="true">(Un)check the boxes to filter the results.</p><input checked="true" class="filter" data-test-result="passed" disabled="true" hidden="true" name="filter_checkbox" onChange="filterTable(this)" type="checkbox"/><span class="passed">0 passed</span>, <input checked="true" class="filter" data-test-result="skipped" disabled="true" hidden="true" name="filter_checkbox" onChange="filterTable(this)" type="checkbox"/><span class="skipped">0 skipped</span>, <input checked="true" class="filter" data-test-result="failed" hidden="true" name="filter_checkbox" onChange="filterTable(this)" type="checkbox"/><span class="failed">3 failed</span>, <input checked="true" class="filter" data-test-result="error" disabled="true" hidden="true" name="filter_checkbox" onChange="filterTable(this)" type="checkbox"/><span class="error">0 errors</span>, <input checked="true" class="filter" data-test-result="xfailed" disabled="true" hidden="true" name="filter_checkbox" onChange="filterTable(this)" type="checkbox"/><span class="xfailed">0 expected failures</span>, <input checked="true" class="filter" data-test-result="xpassed" disabled="true" hidden="true" name="filter_checkbox" onChange="filterTable(this)" type="checkbox"/><span class="xpassed">0 unexpected passes</span>
    <h2>Results</h2>
    <table id="results-table">
      <thead id="results-table-head">
        <tr>
          <th class="sortable result initial-sort" col="result">Result</th>
          <th class="sortable" col="name">Test</th>
          <th class="sortable" col="duration">Duration</th>
          <th class="sortable links" col="links">Links</th></tr>
        <tr hidden="true" id="not-found-message">
          <th colspan="4">No results found. Try to check the filters</th></tr></thead>
      <tbody class="failed results-table-row">
        <tr>
          <td class="col-result">Failed</td>
          <td class="col-name">test_smtp.py::test_bar_fixture</td>
          <td class="col-duration">0.06</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log">testdir = &lt;Testdir local(&#x27;C:\\Users\\Admin\\AppData\\Local\\Temp\\pytest-of-Admin\\pytest-6\\test_bar_fixture0&#x27;)&gt;<br/><br/>    def test_bar_fixture(testdir):<br/>        &quot;&quot;&quot;Make sure that pytest accepts our fixture.&quot;&quot;&quot;<br/>    <br/>        # create a temporary pytest test module<br/>        testdir.makepyfile(&quot;&quot;&quot;<br/>            def test_sth(bar):<br/>                assert bar == &quot;europython2015&quot;<br/>        &quot;&quot;&quot;)<br/>    <br/>        # run pytest with the following cmd args<br/>        result = testdir.runpytest(<br/>            &#x27;--foo=europython2015&#x27;,<br/>            &#x27;-v&#x27;<br/>        )<br/>    <br/>        # fnmatch_lines does an assertion internally<br/>&gt;       result.stdout.fnmatch_lines([<br/>            &#x27;*::test_sth PASSED*&#x27;,<br/>        ])<br/><span class="error">E       Failed: remains unmatched: &#x27;*::test_sth PASSED*&#x27;</span><br/><br/>C:\workspace\pytest-smtp\tests\test_smtp.py:20: Failed<br/> ------------------------------Captured stderr call------------------------------ <br/>ERROR: usage: pytest [options] [file_or_dir] [file_or_dir] [...]
pytest: error: unrecognized arguments: --foo=europython2015
  inifile: None
  rootdir: C:\Users\Admin\AppData\Local\Temp\pytest-of-Admin\pytest-6\test_bar_fixture0

<br/></div></td></tr></tbody>
      <tbody class="failed results-table-row">
        <tr>
          <td class="col-result">Failed</td>
          <td class="col-name">test_smtp.py::test_help_message</td>
          <td class="col-duration">0.13</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log">testdir = &lt;Testdir local(&#x27;C:\\Users\\Admin\\AppData\\Local\\Temp\\pytest-of-Admin\\pytest-6\\test_help_message0&#x27;)&gt;<br/><br/>    def test_help_message(testdir):<br/>        result = testdir.runpytest(<br/>            &#x27;--help&#x27;,<br/>        )<br/>        # fnmatch_lines does an assertion internally<br/>&gt;       result.stdout.fnmatch_lines([<br/>            &#x27;smtp:&#x27;,<br/>            &#x27;*--foo=DEST_FOO*Set the value for the fixture &quot;bar&quot;.&#x27;,<br/>        ])<br/><span class="error">E       Failed: nomatch: &#x27;smtp:&#x27;</span><br/><span class="error">E           and: &#x27;usage: pytest [options] [file_or_dir] [file_or_dir] [...]&#x27;</span><br/><span class="error">E           and: &#x27;&#x27;</span><br/><span class="error">E           and: &#x27;positional arguments:&#x27;</span><br/><span class="error">E           and: &#x27;  file_or_dir&#x27;</span><br/><span class="error">E           and: &#x27;&#x27;</span><br/><span class="error">E           and: &#x27;general:&#x27;</span><br/><span class="error">E           and: &#x27;  -k EXPRESSION         only run tests which match the given substring&#x27;</span><br/><span class="error">E           and: &#x27;                        expression. An expression is a python evaluatable&#x27;</span><br/><span class="error">E           and: &#x27;                        expression where all names are substring-matched against&#x27;</span><br/><span class="error">E           and: &#x27;                        test names and their parent classes. Example: -k&#x27;</span><br/><span class="error">E           and: &quot;                        &#x27;test_method or test_other&#x27; matches all test functions&quot;</span><br/><span class="error">E           and: &quot;                        and classes whose name contains &#x27;test_method&#x27; or&quot;</span><br/><span class="error">E           and: &quot;                        &#x27;test_other&#x27;, while -k &#x27;not test_method&#x27; matches those&quot;</span><br/><span class="error">E           and: &quot;                        that don&#x27;t contain &#x27;test_method&#x27; in their names. -k &#x27;not&quot;</span><br/><span class="error">E           and: &quot;                        test_method and not test_other&#x27; will eliminate the&quot;</span><br/><span class="error">E           and: &#x27;                        matches. Additionally keywords are matched to classes&#x27;</span><br/><span class="error">E           and: &#x27;                        and functions containing extra names in their&#x27;</span><br/><span class="error">E           and: &quot;                        &#x27;extra_keyword_matches&#x27; set, as well as functions which&quot;</span><br/><span class="error">E           and: &#x27;                        have names assigned directly to them. The matching is&#x27;</span><br/><span class="error">E           and: &#x27;                        case-insensitive.&#x27;</span><br/><span class="error">E           and: &#x27;  -m MARKEXPR           only run tests matching given mark expression.&#x27;</span><br/><span class="error">E           and: &quot;                        For example: -m &#x27;mark1 and not mark2&#x27;.&quot;</span><br/><span class="error">E           and: &#x27;  --markers             show markers (builtin, plugin and per-project ones).&#x27;</span><br/><span class="error">E           and: &#x27;  -x, --exitfirst       exit instantly on first error or failed test.&#x27;</span><br/><span class="error">E           and: &#x27;  --fixtures, --funcargs&#x27;</span><br/><span class="error">E           and: &#x27;                        show available fixtures, sorted by plugin appearance&#x27;</span><br/><span class="error">E           and: &quot;                        (fixtures with leading &#x27;_&#x27; are only shown with &#x27;-v&#x27;)&quot;</span><br/><span class="error">E           and: &#x27;  --fixtures-per-test   show fixtures per test&#x27;</span><br/><span class="error">E           and: &#x27;  --pdb                 start the interactive Python debugger on errors or&#x27;</span><br/><span class="error">E           and: &#x27;                        KeyboardInterrupt.&#x27;</span><br/><span class="error">E           and: &#x27;  --pdbcls=modulename:classname&#x27;</span><br/><span class="error">E           and: &#x27;                        start a custom interactive Python debugger on errors.&#x27;</span><br/><span class="error">E           and: &#x27;                        For example:&#x27;</span><br/><span class="error">E           and: &#x27;                        --pdbcls=IPython.terminal.debugger:TerminalPdb&#x27;</span><br/><span class="error">E           and: &#x27;  --trace               Immediately break when running each test.&#x27;</span><br/><span class="error">E           and: &#x27;  --capture=method      per-test capturing method: one of fd|sys|no|tee-sys.&#x27;</span><br/><span class="error">E           and: &#x27;  -s                    shortcut for --capture=no.&#x27;</span><br/><span class="error">E           and: &#x27;  --runxfail            report the results of xfail tests as if they were not&#x27;</span><br/><span class="error">E           and: &#x27;                        marked&#x27;</span><br/><span class="error">E           and: &#x27;  --lf, --last-failed   rerun only the tests that failed at the last run (or all&#x27;</span><br/><span class="error">E           and: &#x27;                        if none failed)&#x27;</span><br/><span class="error">E           and: &#x27;  --ff, --failed-first  run all tests, but run the last failures first.&#x27;</span><br/><span class="error">E           and: &#x27;                        This may re-order tests and thus lead to repeated&#x27;</span><br/><span class="error">E           and: &#x27;                        fixture setup/teardown.&#x27;</span><br/><span class="error">E           and: &#x27;  --nf, --new-first     run tests from new files first, then the rest of the&#x27;</span><br/><span class="error">E           and: &#x27;                        tests sorted by file mtime&#x27;</span><br/><span class="error">E           and: &#x27;  --cache-show=[CACHESHOW]&#x27;</span><br/><span class="error">E           and: &quot;                        show cache contents, don&#x27;t perform collection or tests.&quot;</span><br/><span class="error">E           and: &quot;                        Optional argument: glob (default: &#x27;*&#x27;).&quot;</span><br/><span class="error">E           and: &#x27;  --cache-clear         remove all cache contents at start of test run.&#x27;</span><br/><span class="error">E           and: &#x27;  --lfnf={all,none}, --last-failed-no-failures={all,none}&#x27;</span><br/><span class="error">E           and: &#x27;                        which tests to run with no previously (known) failures.&#x27;</span><br/><span class="error">E           and: &#x27;  --sw, --stepwise      exit on test failure and continue from last failing test&#x27;</span><br/><span class="error">E           and: &#x27;                        next time&#x27;</span><br/><span class="error">E           and: &#x27;  --sw-skip, --stepwise-skip&#x27;</span><br/><span class="error">E           and: &#x27;                        ignore the first failing test but stop on the next&#x27;</span><br/><span class="error">E           and: &#x27;                        failing test&#x27;</span><br/><span class="error">E           and: &#x27;&#x27;</span><br/><span class="error">E           and: &#x27;reporting:&#x27;</span><br/><span class="error">E           and: &#x27;  --durations=N         show N slowest setup/test durations (N=0 for all).&#x27;</span><br/><span class="error">E           and: &#x27;  --durations-min=N     Minimal duration in seconds for inclusion in slowest&#x27;</span><br/><span class="error">E           and: &#x27;                        list. Default 0.005&#x27;</span><br/><span class="error">E           and: &#x27;  -v, --verbose         increase verbosity.&#x27;</span><br/><span class="error">E           and: &#x27;  --no-header           disable header&#x27;</span><br/><span class="error">E           and: &#x27;  --no-summary          disable summary&#x27;</span><br/><span class="error">E           and: &#x27;  -q, --quiet           decrease verbosity.&#x27;</span><br/><span class="error">E           and: &#x27;  --verbosity=VERBOSE   set verbosity. Default is 0.&#x27;</span><br/><span class="error">E           and: &#x27;  -r chars              show extra test summary info as specified by chars:&#x27;</span><br/><span class="error">E           and: &#x27;                        (f)ailed, (E)rror, (s)kipped, (x)failed, (X)passed,&#x27;</span><br/><span class="error">E           and: &#x27;                        (p)assed, (P)assed with output, (a)ll except passed&#x27;</span><br/><span class="error">E           and: &#x27;                        (p/P), or (A)ll. (w)arnings are enabled by default (see&#x27;</span><br/><span class="error">E           and: &quot;                        --disable-warnings), &#x27;N&#x27; can be used to reset the list.&quot;</span><br/><span class="error">E           and: &quot;                        (default: &#x27;fE&#x27;).&quot;</span><br/><span class="error">E           and: &#x27;  --disable-warnings, --disable-pytest-warnings&#x27;</span><br/><span class="error">E           and: &#x27;                        disable warnings summary&#x27;</span><br/><span class="error">E           and: &#x27;  -l, --showlocals      show locals in tracebacks (disabled by default).&#x27;</span><br/><span class="error">E           and: &#x27;  --tb=style            traceback print mode (auto/long/short/line/native/no).&#x27;</span><br/><span class="error">E           and: &#x27;  --show-capture={no,stdout,stderr,log,all}&#x27;</span><br/><span class="error">E           and: &#x27;                        Controls how captured stdout/stderr/log is shown on&#x27;</span><br/><span class="error">E           and: &quot;                        failed tests. Default is &#x27;all&#x27;.&quot;</span><br/><span class="error">E           and: &quot;  --full-trace          don&#x27;t cut any tracebacks (default is to cut).&quot;</span><br/><span class="error">E           and: &#x27;  --color=color         color terminal output (yes/no/auto).&#x27;</span><br/><span class="error">E           and: &#x27;  --code-highlight={yes,no}&#x27;</span><br/><span class="error">E           and: &#x27;                        Whether code should be highlighted (only if --color is&#x27;</span><br/><span class="error">E           and: &#x27;                        also enabled)&#x27;</span><br/><span class="error">E           and: &#x27;  --pastebin=mode       send failed|all info to bpaste.net pastebin service.&#x27;</span><br/><span class="error">E           and: &#x27;  --junit-xml=path      create junit-xml style report file at given path.&#x27;</span><br/><span class="error">E           and: &#x27;  --junit-prefix=str    prepend prefix to classnames in junit-xml output&#x27;</span><br/><span class="error">E           and: &#x27;  --html=path           create html report file at given path.&#x27;</span><br/><span class="error">E           and: &#x27;  --self-contained-html&#x27;</span><br/><span class="error">E           and: &#x27;                        create a self-contained html file containing all&#x27;</span><br/><span class="error">E           and: &#x27;                        necessary styles, scripts, and images - this means that&#x27;</span><br/><span class="error">E           and: &#x27;                        the report may not render or function where CSP&#x27;</span><br/><span class="error">E           and: &#x27;                        restrictions are in place (see&#x27;</span><br/><span class="error">E           and: &#x27;                        https://developer.mozilla.org/docs/Web/Security/CSP)&#x27;</span><br/><span class="error">E           and: &#x27;  --css=path            append given css file content to report style file.&#x27;</span><br/><span class="error">E           and: &#x27;&#x27;</span><br/><span class="error">E           and: &#x27;pytest-warnings:&#x27;</span><br/><span class="error">E           and: &#x27;  -W PYTHONWARNINGS, --pythonwarnings=PYTHONWARNINGS&#x27;</span><br/><span class="error">E           and: &#x27;                        set which warnings to report, see -W option of python&#x27;</span><br/><span class="error">E           and: &#x27;                        itself.&#x27;</span><br/><span class="error">E           and: &#x27;  --maxfail=num         exit after first num failures or errors.&#x27;</span><br/><span class="error">E           and: &#x27;  --strict-config       any warnings encountered while parsing the `pytest`&#x27;</span><br/><span class="error">E           and: &#x27;                        section of the configuration file raise errors.&#x27;</span><br/><span class="error">E           and: &#x27;  --strict-markers      markers not registered in the `markers` section of the&#x27;</span><br/><span class="error">E           and: &#x27;                        configuration file raise errors.&#x27;</span><br/><span class="error">E           and: &#x27;  --strict              (deprecated) alias to --strict-markers.&#x27;</span><br/><span class="error">E           and: &#x27;  -c file               load configuration from `file` instead of trying to&#x27;</span><br/><span class="error">E           and: &#x27;                        locate one of the implicit configuration files.&#x27;</span><br/><span class="error">E           and: &#x27;  --continue-on-collection-errors&#x27;</span><br/><span class="error">E           and: &#x27;                        Force test execution even if collection errors occur.&#x27;</span><br/><span class="error">E           and: &#x27;  --rootdir=ROOTDIR     Define root directory for tests. Can be relative path:&#x27;</span><br/><span class="error">E           and: &quot;                        &#x27;root_dir&#x27;, &#x27;./root_dir&#x27;, &#x27;root_dir/another_dir/&#x27;;&quot;</span><br/><span class="error">E           and: &quot;                        absolute path: &#x27;/home/user/root_dir&#x27;; path with&quot;</span><br/><span class="error">E           and: &quot;                        variables: &#x27;$HOME/root_dir&#x27;.&quot;</span><br/><span class="error">E           and: &#x27;&#x27;</span><br/><span class="error">E           and: &#x27;collection:&#x27;</span><br/><span class="error">E           and: &quot;  --collect-only, --co  only collect tests, don&#x27;t execute them.&quot;</span><br/><span class="error">E           and: &#x27;  --pyargs              try to interpret all arguments as python packages.&#x27;</span><br/><span class="error">E           and: &#x27;  --ignore=path         ignore path during collection (multi-allowed).&#x27;</span><br/><span class="error">E           and: &#x27;  --ignore-glob=path    ignore path pattern during collection (multi-allowed).&#x27;</span><br/><span class="error">E           and: &#x27;  --deselect=nodeid_prefix&#x27;</span><br/><span class="error">E           and: &#x27;                        deselect item (via node id prefix) during collection&#x27;</span><br/><span class="error">E           and: &#x27;                        (multi-allowed).&#x27;</span><br/><span class="error">E           and: &quot;  --confcutdir=dir      only load conftest.py&#x27;s relative to specified dir.&quot;</span><br/><span class="error">E           and: &quot;  --noconftest          Don&#x27;t load any conftest.py files.&quot;</span><br/><span class="error">E           and: &#x27;  --keep-duplicates     Keep duplicate tests.&#x27;</span><br/><span class="error">E           and: &#x27;  --collect-in-virtualenv&#x27;</span><br/><span class="error">E           and: &quot;                        Don&#x27;t ignore tests in a local virtualenv directory&quot;</span><br/><span class="error">E           and: &#x27;  --import-mode={prepend,append,importlib}&#x27;</span><br/><span class="error">E           and: &#x27;                        prepend/append to sys.path when importing test modules&#x27;</span><br/><span class="error">E           and: &#x27;                        and conftest files, default is to prepend.&#x27;</span><br/><span class="error">E           and: &#x27;  --doctest-modules     run doctests in all .py modules&#x27;</span><br/><span class="error">E           and: &#x27;  --doctest-report={none,cdiff,ndiff,udiff,only_first_failure}&#x27;</span><br/><span class="error">E           and: &#x27;                        choose another output format for diffs on doctest&#x27;</span><br/><span class="error">E           and: &#x27;                        failure&#x27;</span><br/><span class="error">E           and: &#x27;  --doctest-glob=pat    doctests file matching pattern, default: test*.txt&#x27;</span><br/><span class="error">E           and: &#x27;  --doctest-ignore-import-errors&#x27;</span><br/><span class="error">E           and: &#x27;                        ignore doctest ImportErrors&#x27;</span><br/><span class="error">E           and: &#x27;  --doctest-continue-on-failure&#x27;</span><br/><span class="error">E           and: &#x27;                        for a given doctest, continue to run after the first&#x27;</span><br/><span class="error">E           and: &#x27;                        failure&#x27;</span><br/><span class="error">E           and: &#x27;&#x27;</span><br/><span class="error">E           and: &#x27;test session debugging and configuration:&#x27;</span><br/><span class="error">E           and: &#x27;  --basetemp=dir        base temporary directory for this test run.(warning:&#x27;</span><br/><span class="error">E           and: &#x27;                        this directory is removed if it exists)&#x27;</span><br/><span class="error">E           and: &#x27;  -V, --version         display pytest version and information about&#x27;</span><br/><span class="error">E           and: &#x27;                        plugins.When given twice, also display information about&#x27;</span><br/><span class="error">E           and: &#x27;                        plugins.&#x27;</span><br/><span class="error">E           and: &#x27;  -h, --help            show help message and configuration info&#x27;</span><br/><span class="error">E           and: &#x27;  -p name               early-load given plugin module name or entry point&#x27;</span><br/><span class="error">E           and: &#x27;                        (multi-allowed).&#x27;</span><br/><span class="error">E           and: &#x27;                        To avoid loading of plugins, use the `no:` prefix, e.g.&#x27;</span><br/><span class="error">E           and: &#x27;                        `no:doctest`.&#x27;</span><br/><span class="error">E           and: &#x27;  --trace-config        trace considerations of conftest.py files.&#x27;</span><br/><span class="error">E           and: &#x27;  --debug               store internal tracing debug information in&#x27;</span><br/><span class="error">E           and: &quot;                        &#x27;pytestdebug.log&#x27;.&quot;</span><br/><span class="error">E           and: &#x27;  -o OVERRIDE_INI, --override-ini=OVERRIDE_INI&#x27;</span><br/><span class="error">E           and: &#x27;                        override ini option with &quot;option=value&quot; style, e.g. `-o&#x27;</span><br/><span class="error">E           and: &#x27;                        xfail_strict=True -o cache_dir=cache`.&#x27;</span><br/><span class="error">E           and: &#x27;  --assert=MODE         Control assertion debugging tools.&#x27;</span><br/><span class="error">E           and: &quot;                        &#x27;plain&#x27; performs no assertion debugging.&quot;</span><br/><span class="error">E           and: &quot;                        &#x27;rewrite&#x27; (the default) rewrites assert statements in&quot;</span><br/><span class="error">E           and: &#x27;                        test modules on import to provide assert expression&#x27;</span><br/><span class="error">E           and: &#x27;                        information.&#x27;</span><br/><span class="error">E           and: &#x27;  --setup-only          only setup fixtures, do not execute tests.&#x27;</span><br/><span class="error">E           and: &#x27;  --setup-show          show setup of fixtures while executing tests.&#x27;</span><br/><span class="error">E           and: &quot;  --setup-plan          show what fixtures and tests would be executed but don&#x27;t&quot;</span><br/><span class="error">E           and: &#x27;                        execute anything.&#x27;</span><br/><span class="error">E           and: &#x27;&#x27;</span><br/><span class="error">E           and: &#x27;logging:&#x27;</span><br/><span class="error">E           and: &#x27;  --log-level=LEVEL     level of messages to catch/display.&#x27;</span><br/><span class="error">E           and: &#x27;                        Not set by default, so it depends on the root/parent log&#x27;</span><br/><span class="error">E           and: &#x27;                        handler\&#x27;s effective level, where it is &quot;WARNING&quot; by&#x27;</span><br/><span class="error">E           and: &#x27;                        default.&#x27;</span><br/><span class="error">E           and: &#x27;  --log-format=LOG_FORMAT&#x27;</span><br/><span class="error">E           and: &#x27;                        log format as used by the logging module.&#x27;</span><br/><span class="error">E           and: &#x27;  --log-date-format=LOG_DATE_FORMAT&#x27;</span><br/><span class="error">E           and: &#x27;                        log date format as used by the logging module.&#x27;</span><br/><span class="error">E           and: &#x27;  --log-cli-level=LOG_CLI_LEVEL&#x27;</span><br/><span class="error">E           and: &#x27;                        cli logging level.&#x27;</span><br/><span class="error">E           and: &#x27;  --log-cli-format=LOG_CLI_FORMAT&#x27;</span><br/><span class="error">E           and: &#x27;                        log format as used by the logging module.&#x27;</span><br/><span class="error">E           and: &#x27;  --log-cli-date-format=LOG_CLI_DATE_FORMAT&#x27;</span><br/><span class="error">E           and: &#x27;                        log date format as used by the logging module.&#x27;</span><br/><span class="error">E           and: &#x27;  --log-file=LOG_FILE   path to a file when logging will be written to.&#x27;</span><br/><span class="error">E           and: &#x27;  --log-file-level=LOG_FILE_LEVEL&#x27;</span><br/><span class="error">E           and: &#x27;                        log file logging level.&#x27;</span><br/><span class="error">E           and: &#x27;  --log-file-format=LOG_FILE_FORMAT&#x27;</span><br/><span class="error">E           and: &#x27;                        log format as used by the logging module.&#x27;</span><br/><span class="error">E           and: &#x27;  --log-file-date-format=LOG_FILE_DATE_FORMAT&#x27;</span><br/><span class="error">E           and: &#x27;                        log date format as used by the logging module.&#x27;</span><br/><span class="error">E           and: &#x27;  --log-auto-indent=LOG_AUTO_INDENT&#x27;</span><br/><span class="error">E           and: &#x27;                        Auto-indent multiline messages passed to the logging&#x27;</span><br/><span class="error">E           and: &#x27;                        module. Accepts true|on, false|off or an integer.&#x27;</span><br/><span class="error">E           and: &#x27;&#x27;</span><br/><span class="error">E           and: &#x27;email:&#x27;</span><br/><span class="error">E           and: &#x27;  --send-email          Send email when --send-email&#x27;</span><br/><span class="error">E           and: &#x27;  --smtp-host=SMTP_HOST&#x27;</span><br/><span class="error">E           and: &#x27;                        SMTP host&#x27;</span><br/><span class="error">E           and: &#x27;  --smtp-port=SMTP_PORT&#x27;</span><br/><span class="error">E           and: &#x27;                        SMTP port&#x27;</span><br/><span class="error">E           and: &#x27;  --smtp-user=SMTP_USER&#x27;</span><br/><span class="error">E           and: &#x27;                        SMTP user&#x27;</span><br/><span class="error">E           and: &#x27;  --smtp-pwd=SMTP_PWD   SMTP password&#x27;</span><br/><span class="error">E           and: &#x27;  --smtp-ssl=SMTP_SSL   Use smtp_ssl&#x27;</span><br/><span class="error">E           and: &#x27;  --email-subject=EMAIL_SUBJECT&#x27;</span><br/><span class="error">E           and: &#x27;                        Email subject&#x27;</span><br/><span class="error">E           and: &#x27;  --email-to=EMAIL_TO   Email receivers, comma-separated&#x27;</span><br/><span class="error">E           and: &#x27;  --email-body=EMAIL_BODY&#x27;</span><br/><span class="error">E           and: &#x27;                        Email content, support HTML&#x27;</span><br/><span class="error">E           and: &#x27;  --email-template=EMAIL_TEMPLATE&#x27;</span><br/><span class="error">E           and: &#x27;                        Email content template path&#x27;</span><br/><span class="error">E           and: &#x27;  --email-attachments=EMAIL_ATTACHMENTS&#x27;</span><br/><span class="error">E           and: &#x27;                        Email attachments, commn-separated&#x27;</span><br/><span class="error">E           and: &#x27;&#x27;</span><br/><span class="error">E           and: &#x27;custom options:&#x27;</span><br/><span class="error">E           and: &#x27;  --metadata=key value  additional metadata.&#x27;</span><br/><span class="error">E           and: &#x27;  --metadata-from-json=METADATA_FROM_JSON&#x27;</span><br/><span class="error">E           and: &#x27;                        additional metadata from a json string.&#x27;</span><br/><span class="error">E           and: &#x27;  --count=COUNT         Number of times to repeat each test&#x27;</span><br/><span class="error">E           and: &#x27;  --repeat-scope={function,class,module,session}&#x27;</span><br/><span class="error">E           and: &#x27;                        Scope for repeating tests&#x27;</span><br/><span class="error">E           and: &#x27;&#x27;</span><br/><span class="error">E           and: &#x27;[pytest] ini-options in the first pytest.ini|tox.ini|setup.cfg file found:&#x27;</span><br/><span class="error">E           and: &#x27;&#x27;</span><br/><span class="error">E           and: &#x27;  markers (linelist):   markers for test functions&#x27;</span><br/><span class="error">E           and: &#x27;  empty_parameter_set_mark (string):&#x27;</span><br/><span class="error">E           and: &#x27;                        default marker for empty parametersets&#x27;</span><br/><span class="error">E           and: &#x27;  norecursedirs (args): directory patterns to avoid for recursion&#x27;</span><br/><span class="error">E           and: &#x27;  testpaths (args):     directories to search for tests when no files or&#x27;</span><br/><span class="error">E           and: &#x27;                        directories are given in the command line.&#x27;</span><br/><span class="error">E           and: &#x27;  filterwarnings (linelist):&#x27;</span><br/><span class="error">E           and: &#x27;                        Each line specifies a pattern for&#x27;</span><br/><span class="error">E           and: &#x27;                        warnings.filterwarnings. Processed after&#x27;</span><br/><span class="error">E           and: &#x27;                        -W/--pythonwarnings.&#x27;</span><br/><span class="error">E           and: &#x27;  usefixtures (args):   list of default fixtures to be used with this project&#x27;</span><br/><span class="error">E           and: &#x27;  python_files (args):  glob-style file patterns for Python test module&#x27;</span><br/><span class="error">E           and: &#x27;                        discovery&#x27;</span><br/><span class="error">E           and: &#x27;  python_classes (args):&#x27;</span><br/><span class="error">E           and: &#x27;                        prefixes or glob names for Python test class discovery&#x27;</span><br/><span class="error">E           and: &#x27;  python_functions (args):&#x27;</span><br/><span class="error">E           and: &#x27;                        prefixes or glob names for Python test function and&#x27;</span><br/><span class="error">E           and: &#x27;                        method discovery&#x27;</span><br/><span class="error">E           and: &#x27;  disable_test_id_escaping_and_forfeit_all_rights_to_community_support (bool):&#x27;</span><br/><span class="error">E           and: &#x27;                        disable string escape non-ascii characters, might cause&#x27;</span><br/><span class="error">E           and: &#x27;                        unwanted side effects(use at your own risk)&#x27;</span><br/><span class="error">E           and: &#x27;  console_output_style (string):&#x27;</span><br/><span class="error">E           and: &#x27;                        console output: &quot;classic&quot;, or with additional progress&#x27;</span><br/><span class="error">E           and: &#x27;                        information (&quot;progress&quot; (percentage) | &quot;count&quot;).&#x27;</span><br/><span class="error">E           and: &#x27;  xfail_strict (bool):  default for the strict parameter of xfail markers when&#x27;</span><br/><span class="error">E           and: &#x27;                        not given explicitly (default: False)&#x27;</span><br/><span class="error">E           and: &#x27;  enable_assertion_pass_hook (bool):&#x27;</span><br/><span class="error">E           and: &#x27;                        Enables the pytest_assertion_pass hook.Make sure to&#x27;</span><br/><span class="error">E           and: &#x27;                        delete any previously generated pyc cache files.&#x27;</span><br/><span class="error">E           and: &#x27;  junit_suite_name (string):&#x27;</span><br/><span class="error">E           and: &#x27;                        Test suite name for JUnit report&#x27;</span><br/><span class="error">E           and: &#x27;  junit_logging (string):&#x27;</span><br/><span class="error">E           and: &#x27;                        Write captured log messages to JUnit report: one of&#x27;</span><br/><span class="error">E           and: &#x27;                        no|log|system-out|system-err|out-err|all&#x27;</span><br/><span class="error">E           and: &#x27;  junit_log_passing_tests (bool):&#x27;</span><br/><span class="error">E           and: &#x27;                        Capture log information for passing tests to JUnit&#x27;</span><br/><span class="error">E           and: &#x27;                        report:&#x27;</span><br/><span class="error">E           and: &#x27;  junit_duration_report (string):&#x27;</span><br/><span class="error">E           and: &#x27;                        Duration time to report: one of total|call&#x27;</span><br/><span class="error">E           and: &#x27;  junit_family (string):&#x27;</span><br/><span class="error">E           and: &#x27;                        Emit XML for schema: one of legacy|xunit1|xunit2&#x27;</span><br/><span class="error">E           and: &#x27;  doctest_optionflags (args):&#x27;</span><br/><span class="error">E           and: &#x27;                        option flags for doctests&#x27;</span><br/><span class="error">E           and: &#x27;  doctest_encoding (string):&#x27;</span><br/><span class="error">E           and: &#x27;                        encoding used for doctest files&#x27;</span><br/><span class="error">E           and: &#x27;  cache_dir (string):   cache directory path.&#x27;</span><br/><span class="error">E           and: &#x27;  log_level (string):   default value for --log-level&#x27;</span><br/><span class="error">E           and: &#x27;  log_format (string):  default value for --log-format&#x27;</span><br/><span class="error">E           and: &#x27;  log_date_format (string):&#x27;</span><br/><span class="error">E           and: &#x27;                        default value for --log-date-format&#x27;</span><br/><span class="error">E           and: &#x27;  log_cli (bool):       enable log display during test run (also known as &quot;live&#x27;</span><br/><span class="error">E           and: &#x27;                        logging&quot;).&#x27;</span><br/><span class="error">E           and: &#x27;  log_cli_level (string):&#x27;</span><br/><span class="error">E           and: &#x27;                        default value for --log-cli-level&#x27;</span><br/><span class="error">E           and: &#x27;  log_cli_format (string):&#x27;</span><br/><span class="error">E           and: &#x27;                        default value for --log-cli-format&#x27;</span><br/><span class="error">E           and: &#x27;  log_cli_date_format (string):&#x27;</span><br/><span class="error">E           and: &#x27;                        default value for --log-cli-date-format&#x27;</span><br/><span class="error">E           and: &#x27;  log_file (string):    default value for --log-file&#x27;</span><br/><span class="error">E           and: &#x27;  log_file_level (string):&#x27;</span><br/><span class="error">E           and: &#x27;                        default value for --log-file-level&#x27;</span><br/><span class="error">E           and: &#x27;  log_file_format (string):&#x27;</span><br/><span class="error">E           and: &#x27;                        default value for --log-file-format&#x27;</span><br/><span class="error">E           and: &#x27;  log_file_date_format (string):&#x27;</span><br/><span class="error">E           and: &#x27;                        default value for --log-file-date-format&#x27;</span><br/><span class="error">E           and: &#x27;  log_auto_indent (string):&#x27;</span><br/><span class="error">E           and: &#x27;                        default value for --log-auto-indent&#x27;</span><br/><span class="error">E           and: &#x27;  faulthandler_timeout (string):&#x27;</span><br/><span class="error">E           and: &#x27;                        Dump the traceback of all threads if a test takes more&#x27;</span><br/><span class="error">E           and: &#x27;                        than TIMEOUT seconds to finish.&#x27;</span><br/><span class="error">E           and: &#x27;  addopts (args):       extra command line options&#x27;</span><br/><span class="error">E           and: &#x27;  minversion (string):  minimally required pytest version&#x27;</span><br/><span class="error">E           and: &#x27;  required_plugins (args):&#x27;</span><br/><span class="error">E           and: &#x27;                        plugins that must be present for pytest to run&#x27;</span><br/><span class="error">E           and: &#x27;  render_collapsed (bool):&#x27;</span><br/><span class="error">E           and: &#x27;                        Open the report with all rows collapsed. Useful for very&#x27;</span><br/><span class="error">E           and: &#x27;                        large reports&#x27;</span><br/><span class="error">E           and: &#x27;  max_asset_filename_length (string):&#x27;</span><br/><span class="error">E           and: &#x27;                        set the maximum filename length for assets attached to&#x27;</span><br/><span class="error">E           and: &#x27;                        the html report.&#x27;</span><br/><span class="error">E           and: &#x27;  smtp_host (string):   SMTP host&#x27;</span><br/><span class="error">E           and: &#x27;  smtp_port (string):   SMTP port&#x27;</span><br/><span class="error">E           and: &#x27;  smtp_user (string):   SMTP user&#x27;</span><br/><span class="error">E           and: &#x27;  smtp_pwd (string):    SMTP password&#x27;</span><br/><span class="error">E           and: &#x27;  smtp_ssl (string):    Use SMTP_SSL&#x27;</span><br/><span class="error">E           and: &#x27;  email_subject (string):&#x27;</span><br/><span class="error">E           and: &#x27;                        Email subject&#x27;</span><br/><span class="error">E           and: &#x27;  email_to (string):    Email receivers, comma-separated&#x27;</span><br/><span class="error">E           and: &#x27;  email_body (string):  Email content, support html&#x27;</span><br/><span class="error">E           and: &#x27;  email_template (string):&#x27;</span><br/><span class="error">E           and: &#x27;                        Email content template path&#x27;</span><br/><span class="error">E           and: &#x27;  email_attachments (string):&#x27;</span><br/><span class="error">E           and: &#x27;                        Email attachments, commn-separated&#x27;</span><br/><span class="error">E           and: &#x27;&#x27;</span><br/><span class="error">E           and: &#x27;environment variables:&#x27;</span><br/><span class="error">E           and: &#x27;  PYTEST_ADDOPTS           extra command line options&#x27;</span><br/><span class="error">E           and: &#x27;  PYTEST_PLUGINS           comma-separated plugins to load during startup&#x27;</span><br/><span class="error">E           and: &#x27;  PYTEST_DISABLE_PLUGIN_AUTOLOAD set to disable plugin auto-loading&#x27;</span><br/><span class="error">E           and: &quot;  PYTEST_DEBUG             set to enable debug tracing of pytest&#x27;s internals&quot;</span><br/><span class="error">E           and: &#x27;&#x27;</span><br/><span class="error">E           and: &#x27;&#x27;</span><br/><span class="error">E           and: &#x27;to see available markers type: pytest --markers&#x27;</span><br/><span class="error">E           and: &#x27;to see available fixtures type: pytest --fixtures&#x27;</span><br/><span class="error">E           and: &quot;(shown according to specified file_or_dir or current dir if not specified; fixtures with leading &#x27;_&#x27; are only shown with the &#x27;-v&#x27; option&quot;</span><br/><span class="error">E           and: &#x27;warning : C:\\Users\\Admin\\.virtualenvs\\pytest-smtp\\lib\\site-packages\\_pytest\\config\\__init__.py:1114: PytestAssertRewriteWarning: Module already imported so cannot be rewritten: pytest_smtp&#x27;</span><br/><span class="error">E           and: &#x27;  self._mark_plugins_for_rewrite(hook)&#x27;</span><br/><span class="error">E           and: &#x27;&#x27;</span><br/><span class="error">E       remains unmatched: &#x27;smtp:&#x27;</span><br/><br/>C:\workspace\pytest-smtp\tests\test_smtp.py:33: Failed<br/> ------------------------------Captured stdout call------------------------------ <br/>usage: pytest [options] [file_or_dir] [file_or_dir] [...]

positional arguments:
  file_or_dir

general:
  -k EXPRESSION         only run tests which match the given substring
                        expression. An expression is a python evaluatable
                        expression where all names are substring-matched against
                        test names and their parent classes. Example: -k
                        &#x27;test_method or test_other&#x27; matches all test functions
                        and classes whose name contains &#x27;test_method&#x27; or
                        &#x27;test_other&#x27;, while -k &#x27;not test_method&#x27; matches those
                        that don&#x27;t contain &#x27;test_method&#x27; in their names. -k &#x27;not
                        test_method and not test_other&#x27; will eliminate the
                        matches. Additionally keywords are matched to classes
                        and functions containing extra names in their
                        &#x27;extra_keyword_matches&#x27; set, as well as functions which
                        have names assigned directly to them. The matching is
                        case-insensitive.
  -m MARKEXPR           only run tests matching given mark expression.
                        For example: -m &#x27;mark1 and not mark2&#x27;.
  --markers             show markers (builtin, plugin and per-project ones).
  -x, --exitfirst       exit instantly on first error or failed test.
  --fixtures, --funcargs
                        show available fixtures, sorted by plugin appearance
                        (fixtures with leading &#x27;_&#x27; are only shown with &#x27;-v&#x27;)
  --fixtures-per-test   show fixtures per test
  --pdb                 start the interactive Python debugger on errors or
                        KeyboardInterrupt.
  --pdbcls=modulename:classname
                        start a custom interactive Python debugger on errors.
                        For example:
                        --pdbcls=IPython.terminal.debugger:TerminalPdb
  --trace               Immediately break when running each test.
  --capture=method      per-test capturing method: one of fd|sys|no|tee-sys.
  -s                    shortcut for --capture=no.
  --runxfail            report the results of xfail tests as if they were not
                        marked
  --lf, --last-failed   rerun only the tests that failed at the last run (or all
                        if none failed)
  --ff, --failed-first  run all tests, but run the last failures first.
                        This may re-order tests and thus lead to repeated
                        fixture setup/teardown.
  --nf, --new-first     run tests from new files first, then the rest of the
                        tests sorted by file mtime
  --cache-show=[CACHESHOW]
                        show cache contents, don&#x27;t perform collection or tests.
                        Optional argument: glob (default: &#x27;*&#x27;).
  --cache-clear         remove all cache contents at start of test run.
  --lfnf={all,none}, --last-failed-no-failures={all,none}
                        which tests to run with no previously (known) failures.
  --sw, --stepwise      exit on test failure and continue from last failing test
                        next time
  --sw-skip, --stepwise-skip
                        ignore the first failing test but stop on the next
                        failing test

reporting:
  --durations=N         show N slowest setup/test durations (N=0 for all).
  --durations-min=N     Minimal duration in seconds for inclusion in slowest
                        list. Default 0.005
  -v, --verbose         increase verbosity.
  --no-header           disable header
  --no-summary          disable summary
  -q, --quiet           decrease verbosity.
  --verbosity=VERBOSE   set verbosity. Default is 0.
  -r chars              show extra test summary info as specified by chars:
                        (f)ailed, (E)rror, (s)kipped, (x)failed, (X)passed,
                        (p)assed, (P)assed with output, (a)ll except passed
                        (p/P), or (A)ll. (w)arnings are enabled by default (see
                        --disable-warnings), &#x27;N&#x27; can be used to reset the list.
                        (default: &#x27;fE&#x27;).
  --disable-warnings, --disable-pytest-warnings
                        disable warnings summary
  -l, --showlocals      show locals in tracebacks (disabled by default).
  --tb=style            traceback print mode (auto/long/short/line/native/no).
  --show-capture={no,stdout,stderr,log,all}
                        Controls how captured stdout/stderr/log is shown on
                        failed tests. Default is &#x27;all&#x27;.
  --full-trace          don&#x27;t cut any tracebacks (default is to cut).
  --color=color         color terminal output (yes/no/auto).
  --code-highlight={yes,no}
                        Whether code should be highlighted (only if --color is
                        also enabled)
  --pastebin=mode       send failed|all info to bpaste.net pastebin service.
  --junit-xml=path      create junit-xml style report file at given path.
  --junit-prefix=str    prepend prefix to classnames in junit-xml output
  --html=path           create html report file at given path.
  --self-contained-html
                        create a self-contained html file containing all
                        necessary styles, scripts, and images - this means that
                        the report may not render or function where CSP
                        restrictions are in place (see
                        https://developer.mozilla.org/docs/Web/Security/CSP)
  --css=path            append given css file content to report style file.

pytest-warnings:
  -W PYTHONWARNINGS, --pythonwarnings=PYTHONWARNINGS
                        set which warnings to report, see -W option of python
                        itself.
  --maxfail=num         exit after first num failures or errors.
  --strict-config       any warnings encountered while parsing the `pytest`
                        section of the configuration file raise errors.
  --strict-markers      markers not registered in the `markers` section of the
                        configuration file raise errors.
  --strict              (deprecated) alias to --strict-markers.
  -c file               load configuration from `file` instead of trying to
                        locate one of the implicit configuration files.
  --continue-on-collection-errors
                        Force test execution even if collection errors occur.
  --rootdir=ROOTDIR     Define root directory for tests. Can be relative path:
                        &#x27;root_dir&#x27;, &#x27;./root_dir&#x27;, &#x27;root_dir/another_dir/&#x27;;
                        absolute path: &#x27;/home/user/root_dir&#x27;; path with
                        variables: &#x27;$HOME/root_dir&#x27;.

collection:
  --collect-only, --co  only collect tests, don&#x27;t execute them.
  --pyargs              try to interpret all arguments as python packages.
  --ignore=path         ignore path during collection (multi-allowed).
  --ignore-glob=path    ignore path pattern during collection (multi-allowed).
  --deselect=nodeid_prefix
                        deselect item (via node id prefix) during collection
                        (multi-allowed).
  --confcutdir=dir      only load conftest.py&#x27;s relative to specified dir.
  --noconftest          Don&#x27;t load any conftest.py files.
  --keep-duplicates     Keep duplicate tests.
  --collect-in-virtualenv
                        Don&#x27;t ignore tests in a local virtualenv directory
  --import-mode={prepend,append,importlib}
                        prepend/append to sys.path when importing test modules
                        and conftest files, default is to prepend.
  --doctest-modules     run doctests in all .py modules
  --doctest-report={none,cdiff,ndiff,udiff,only_first_failure}
                        choose another output format for diffs on doctest
                        failure
  --doctest-glob=pat    doctests file matching pattern, default: test*.txt
  --doctest-ignore-import-errors
                        ignore doctest ImportErrors
  --doctest-continue-on-failure
                        for a given doctest, continue to run after the first
                        failure

test session debugging and configuration:
  --basetemp=dir        base temporary directory for this test run.(warning:
                        this directory is removed if it exists)
  -V, --version         display pytest version and information about
                        plugins.When given twice, also display information about
                        plugins.
  -h, --help            show help message and configuration info
  -p name               early-load given plugin module name or entry point
                        (multi-allowed).
                        To avoid loading of plugins, use the `no:` prefix, e.g.
                        `no:doctest`.
  --trace-config        trace considerations of conftest.py files.
  --debug               store internal tracing debug information in
                        &#x27;pytestdebug.log&#x27;.
  -o OVERRIDE_INI, --override-ini=OVERRIDE_INI
                        override ini option with &quot;option=value&quot; style, e.g. `-o
                        xfail_strict=True -o cache_dir=cache`.
  --assert=MODE         Control assertion debugging tools.
                        &#x27;plain&#x27; performs no assertion debugging.
                        &#x27;rewrite&#x27; (the default) rewrites assert statements in
                        test modules on import to provide assert expression
                        information.
  --setup-only          only setup fixtures, do not execute tests.
  --setup-show          show setup of fixtures while executing tests.
  --setup-plan          show what fixtures and tests would be executed but don&#x27;t
                        execute anything.

logging:
  --log-level=LEVEL     level of messages to catch/display.
                        Not set by default, so it depends on the root/parent log
                        handler&#x27;s effective level, where it is &quot;WARNING&quot; by
                        default.
  --log-format=LOG_FORMAT
                        log format as used by the logging module.
  --log-date-format=LOG_DATE_FORMAT
                        log date format as used by the logging module.
  --log-cli-level=LOG_CLI_LEVEL
                        cli logging level.
  --log-cli-format=LOG_CLI_FORMAT
                        log format as used by the logging module.
  --log-cli-date-format=LOG_CLI_DATE_FORMAT
                        log date format as used by the logging module.
  --log-file=LOG_FILE   path to a file when logging will be written to.
  --log-file-level=LOG_FILE_LEVEL
                        log file logging level.
  --log-file-format=LOG_FILE_FORMAT
                        log format as used by the logging module.
  --log-file-date-format=LOG_FILE_DATE_FORMAT
                        log date format as used by the logging module.
  --log-auto-indent=LOG_AUTO_INDENT
                        Auto-indent multiline messages passed to the logging
                        module. Accepts true|on, false|off or an integer.

email:
  --send-email          Send email when --send-email
  --smtp-host=SMTP_HOST
                        SMTP host
  --smtp-port=SMTP_PORT
                        SMTP port
  --smtp-user=SMTP_USER
                        SMTP user
  --smtp-pwd=SMTP_PWD   SMTP password
  --smtp-ssl=SMTP_SSL   Use smtp_ssl
  --email-subject=EMAIL_SUBJECT
                        Email subject
  --email-to=EMAIL_TO   Email receivers, comma-separated
  --email-body=EMAIL_BODY
                        Email content, support HTML
  --email-template=EMAIL_TEMPLATE
                        Email content template path
  --email-attachments=EMAIL_ATTACHMENTS
                        Email attachments, commn-separated

custom options:
  --metadata=key value  additional metadata.
  --metadata-from-json=METADATA_FROM_JSON
                        additional metadata from a json string.
  --count=COUNT         Number of times to repeat each test
  --repeat-scope={function,class,module,session}
                        Scope for repeating tests

[pytest] ini-options in the first pytest.ini|tox.ini|setup.cfg file found:

  markers (linelist):   markers for test functions
  empty_parameter_set_mark (string):
                        default marker for empty parametersets
  norecursedirs (args): directory patterns to avoid for recursion
  testpaths (args):     directories to search for tests when no files or
                        directories are given in the command line.
  filterwarnings (linelist):
                        Each line specifies a pattern for
                        warnings.filterwarnings. Processed after
                        -W/--pythonwarnings.
  usefixtures (args):   list of default fixtures to be used with this project
  python_files (args):  glob-style file patterns for Python test module
                        discovery
  python_classes (args):
                        prefixes or glob names for Python test class discovery
  python_functions (args):
                        prefixes or glob names for Python test function and
                        method discovery
  disable_test_id_escaping_and_forfeit_all_rights_to_community_support (bool):
                        disable string escape non-ascii characters, might cause
                        unwanted side effects(use at your own risk)
  console_output_style (string):
                        console output: &quot;classic&quot;, or with additional progress
                        information (&quot;progress&quot; (percentage) | &quot;count&quot;).
  xfail_strict (bool):  default for the strict parameter of xfail markers when
                        not given explicitly (default: False)
  enable_assertion_pass_hook (bool):
                        Enables the pytest_assertion_pass hook.Make sure to
                        delete any previously generated pyc cache files.
  junit_suite_name (string):
                        Test suite name for JUnit report
  junit_logging (string):
                        Write captured log messages to JUnit report: one of
                        no|log|system-out|system-err|out-err|all
  junit_log_passing_tests (bool):
                        Capture log information for passing tests to JUnit
                        report:
  junit_duration_report (string):
                        Duration time to report: one of total|call
  junit_family (string):
                        Emit XML for schema: one of legacy|xunit1|xunit2
  doctest_optionflags (args):
                        option flags for doctests
  doctest_encoding (string):
                        encoding used for doctest files
  cache_dir (string):   cache directory path.
  log_level (string):   default value for --log-level
  log_format (string):  default value for --log-format
  log_date_format (string):
                        default value for --log-date-format
  log_cli (bool):       enable log display during test run (also known as &quot;live
                        logging&quot;).
  log_cli_level (string):
                        default value for --log-cli-level
  log_cli_format (string):
                        default value for --log-cli-format
  log_cli_date_format (string):
                        default value for --log-cli-date-format
  log_file (string):    default value for --log-file
  log_file_level (string):
                        default value for --log-file-level
  log_file_format (string):
                        default value for --log-file-format
  log_file_date_format (string):
                        default value for --log-file-date-format
  log_auto_indent (string):
                        default value for --log-auto-indent
  faulthandler_timeout (string):
                        Dump the traceback of all threads if a test takes more
                        than TIMEOUT seconds to finish.
  addopts (args):       extra command line options
  minversion (string):  minimally required pytest version
  required_plugins (args):
                        plugins that must be present for pytest to run
  render_collapsed (bool):
                        Open the report with all rows collapsed. Useful for very
                        large reports
  max_asset_filename_length (string):
                        set the maximum filename length for assets attached to
                        the html report.
  smtp_host (string):   SMTP host
  smtp_port (string):   SMTP port
  smtp_user (string):   SMTP user
  smtp_pwd (string):    SMTP password
  smtp_ssl (string):    Use SMTP_SSL
  email_subject (string):
                        Email subject
  email_to (string):    Email receivers, comma-separated
  email_body (string):  Email content, support html
  email_template (string):
                        Email content template path
  email_attachments (string):
                        Email attachments, commn-separated

environment variables:
  PYTEST_ADDOPTS           extra command line options
  PYTEST_PLUGINS           comma-separated plugins to load during startup
  PYTEST_DISABLE_PLUGIN_AUTOLOAD set to disable plugin auto-loading
  PYTEST_DEBUG             set to enable debug tracing of pytest&#x27;s internals


to see available markers type: pytest --markers
to see available fixtures type: pytest --fixtures
(shown according to specified file_or_dir or current dir if not specified; fixtures with leading &#x27;_&#x27; are only shown with the &#x27;-v&#x27; option
warning : C:\Users\Admin\.virtualenvs\pytest-smtp\lib\site-packages\_pytest\config\__init__.py:1114: PytestAssertRewriteWarning: Module already imported so cannot be rewritten: pytest_smtp
  self._mark_plugins_for_rewrite(hook)

<br/></div></td></tr></tbody>
      <tbody class="failed results-table-row">
        <tr>
          <td class="col-result">Failed</td>
          <td class="col-name">test_smtp.py::test_hello_ini_setting</td>
          <td class="col-duration">0.15</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log">testdir = &lt;Testdir local(&#x27;C:\\Users\\Admin\\AppData\\Local\\Temp\\pytest-of-Admin\\pytest-6\\test_hello_ini_setting0&#x27;)&gt;<br/><br/>    def test_hello_ini_setting(testdir):<br/>        testdir.makeini(&quot;&quot;&quot;<br/>            [pytest]<br/>            HELLO = world<br/>        &quot;&quot;&quot;)<br/>    <br/>        testdir.makepyfile(&quot;&quot;&quot;<br/>            import pytest<br/>    <br/>            @pytest.fixture<br/>            def hello(request):<br/>                return request.config.getini(&#x27;HELLO&#x27;)<br/>    <br/>            def test_hello_world(hello):<br/>                assert hello == &#x27;world&#x27;<br/>        &quot;&quot;&quot;)<br/>    <br/>        result = testdir.runpytest(&#x27;-v&#x27;)<br/>    <br/>        # fnmatch_lines does an assertion internally<br/>&gt;       result.stdout.fnmatch_lines([<br/>            &#x27;*::test_hello_world PASSED*&#x27;,<br/>        ])<br/><span class="error">E       Failed: nomatch: &#x27;*::test_hello_world PASSED*&#x27;</span><br/><span class="error">E           and: &#x27;============================= test session starts =============================&#x27;</span><br/><span class="error">E           and: &#x27;platform win32 -- Python 3.9.0, pytest-6.2.2, py-1.10.0, pluggy-0.13.1 -- C:\\Users\\Admin\\.virtualenvs\\pytest-smtp\\Scripts\\python.exe&#x27;</span><br/><span class="error">E           and: &#x27;cachedir: .pytest_cache&#x27;</span><br/><span class="error">E           and: &quot;metadata: {&#x27;Python&#x27;: &#x27;3.9.0&#x27;, &#x27;Platform&#x27;: &#x27;Windows-10-10.0.19041-SP0&#x27;, &#x27;Packages&#x27;: {&#x27;pytest&#x27;: &#x27;6.2.2&#x27;, &#x27;py&#x27;: &#x27;1.10.0&#x27;, &#x27;pluggy&#x27;: &#x27;0.13.1&#x27;}, &#x27;Plugins&#x27;: {&#x27;html&#x27;: &#x27;3.1.1&#x27;, &#x27;metadata&#x27;: &#x27;1.11.0&#x27;, &#x27;repeat&#x27;: &#x27;0.9.1&#x27;, &#x27;smtp&#x27;: &#x27;0.1&#x27;}}&quot;</span><br/><span class="error">E           and: &#x27;rootdir: C:\\Users\\Admin\\AppData\\Local\\Temp\\pytest-of-Admin\\pytest-6\\test_hello_ini_setting0, configfile: tox.ini&#x27;</span><br/><span class="error">E           and: &#x27;plugins: html-3.1.1, metadata-1.11.0, repeat-0.9.1, smtp-0.1&#x27;</span><br/><span class="error">E           and: &#x27;collecting ... collected 1 item&#x27;</span><br/><span class="error">E           and: &#x27;&#x27;</span><br/><span class="error">E           and: &#x27;test_hello_ini_setting.py::test_hello_world ERROR                        [100%]&#x27;</span><br/><span class="error">E           and: &#x27;&#x27;</span><br/><span class="error">E           and: &#x27;=================================== ERRORS ====================================&#x27;</span><br/><span class="error">E           and: &#x27;_____________________ ERROR at setup of test_hello_world ______________________&#x27;</span><br/><span class="error">E           and: &#x27;&#x27;</span><br/><span class="error">E           and: &quot;self = &lt;_pytest.config.Config object at 0x000001F01EA416A0&gt;, name = &#x27;HELLO&#x27;&quot;</span><br/><span class="error">E           and: &#x27;&#x27;</span><br/><span class="error">E           and: &#x27;    def getini(self, name: str):&#x27;</span><br/><span class="error">E           and: &#x27;        &quot;&quot;&quot;Return configuration value from an :ref:`ini file &lt;configfiles&gt;`.&#x27;</span><br/><span class="error">E           and: &#x27;    &#x27;</span><br/><span class="error">E           and: &quot;        If the specified name hasn&#x27;t been registered through a prior&quot;</span><br/><span class="error">E           and: &#x27;        :py:func:`parser.addini &lt;_pytest.config.argparsing.Parser.addini&gt;`&#x27;</span><br/><span class="error">E           and: &#x27;        call (usually from a plugin), a ValueError is raised.&#x27;</span><br/><span class="error">E           and: &#x27;        &quot;&quot;&quot;&#x27;</span><br/><span class="error">E           and: &#x27;        try:&#x27;</span><br/><span class="error">E           and: &#x27;&gt;           return self._inicache[name]&#x27;</span><br/><span class="error">E           and: &quot;E           KeyError: &#x27;HELLO&#x27;&quot;</span><br/><span class="error">E           and: &#x27;&#x27;</span><br/><span class="error">E           and: &#x27;C:\\Users\\Admin\\.virtualenvs\\pytest-smtp\\lib\\site-packages\\_pytest\\config\\__init__.py:1359: KeyError&#x27;</span><br/><span class="error">E           and: &#x27;&#x27;</span><br/><span class="error">E           and: &#x27;During handling of the above exception, another exception occurred:&#x27;</span><br/><span class="error">E           and: &#x27;&#x27;</span><br/><span class="error">E           and: &quot;self = &lt;_pytest.config.Config object at 0x000001F01EA416A0&gt;, name = &#x27;HELLO&#x27;&quot;</span><br/><span class="error">E           and: &#x27;&#x27;</span><br/><span class="error">E           and: &#x27;    def _getini(self, name: str):&#x27;</span><br/><span class="error">E           and: &#x27;        try:&#x27;</span><br/><span class="error">E           and: &#x27;&gt;           description, type, default = self._parser._inidict[name]&#x27;</span><br/><span class="error">E           and: &quot;E           KeyError: &#x27;HELLO&#x27;&quot;</span><br/><span class="error">E           and: &#x27;&#x27;</span><br/><span class="error">E           and: &#x27;C:\\Users\\Admin\\.virtualenvs\\pytest-smtp\\lib\\site-packages\\_pytest\\config\\__init__.py:1366: KeyError&#x27;</span><br/><span class="error">E           and: &#x27;&#x27;</span><br/><span class="error">E           and: &#x27;The above exception was the direct cause of the following exception:&#x27;</span><br/><span class="error">E           and: &#x27;&#x27;</span><br/><span class="error">E           and: &quot;request = &lt;SubRequest &#x27;hello&#x27; for &lt;Function test_hello_world&gt;&gt;&quot;</span><br/><span class="error">E           and: &#x27;&#x27;</span><br/><span class="error">E           and: &#x27;    @pytest.fixture&#x27;</span><br/><span class="error">E           and: &#x27;    def hello(request):&#x27;</span><br/><span class="error">E           and: &quot;&gt;       return request.config.getini(&#x27;HELLO&#x27;)&quot;</span><br/><span class="error">E           and: &#x27;&#x27;</span><br/><span class="error">E           and: &#x27;test_hello_ini_setting.py:5: &#x27;</span><br/><span class="error">E           and: &#x27;_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _&#x27;</span><br/><span class="error">E           and: &#x27;C:\\Users\\Admin\\.virtualenvs\\pytest-smtp\\lib\\site-packages\\_pytest\\config\\__init__.py:1361: in getini&#x27;</span><br/><span class="error">E           and: &#x27;    self._inicache[name] = val = self._getini(name)&#x27;</span><br/><span class="error">E           and: &#x27;_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _&#x27;</span><br/><span class="error">E           and: &#x27;&#x27;</span><br/><span class="error">E           and: &quot;self = &lt;_pytest.config.Config object at 0x000001F01EA416A0&gt;, name = &#x27;HELLO&#x27;&quot;</span><br/><span class="error">E           and: &#x27;&#x27;</span><br/><span class="error">E           and: &#x27;    def _getini(self, name: str):&#x27;</span><br/><span class="error">E           and: &#x27;        try:&#x27;</span><br/><span class="error">E           and: &#x27;            description, type, default = self._parser._inidict[name]&#x27;</span><br/><span class="error">E           and: &#x27;        except KeyError as e:&#x27;</span><br/><span class="error">E           and: &#x27;&gt;           raise ValueError(f&quot;unknown configuration value: {name!r}&quot;) from e&#x27;</span><br/><span class="error">E           and: &quot;E           ValueError: unknown configuration value: &#x27;HELLO&#x27;&quot;</span><br/><span class="error">E           and: &#x27;&#x27;</span><br/><span class="error">E           and: &#x27;C:\\Users\\Admin\\.virtualenvs\\pytest-smtp\\lib\\site-packages\\_pytest\\config\\__init__.py:1368: ValueError&#x27;</span><br/><span class="error">E           and: &#x27;============================== warnings summary ===============================&#x27;</span><br/><span class="error">E           and: &#x27;..\\..\\..\\..\\..\\..\\.virtualenvs\\pytest-smtp\\lib\\site-packages\\_pytest\\config\\__init__.py:1114&#x27;</span><br/><span class="error">E           and: &#x27;  C:\\Users\\Admin\\.virtualenvs\\pytest-smtp\\lib\\site-packages\\_pytest\\config\\__init__.py:1114: PytestAssertRewriteWarning: Module already imported so cannot be rewritten: pytest_smtp&#x27;</span><br/><span class="error">E           and: &#x27;    self._mark_plugins_for_rewrite(hook)&#x27;</span><br/><span class="error">E           and: &#x27;&#x27;</span><br/><span class="error">E           and: &#x27;..\\..\\..\\..\\..\\..\\.virtualenvs\\pytest-smtp\\lib\\site-packages\\_pytest\\config\\__init__.py:1233&#x27;</span><br/><span class="error">E           and: &#x27;  C:\\Users\\Admin\\.virtualenvs\\pytest-smtp\\lib\\site-packages\\_pytest\\config\\__init__.py:1233: PytestConfigWarning: Unknown config option: HELLO&#x27;</span><br/><span class="error">E           and: &#x27;  &#x27;</span><br/><span class="error">E           and: &#x27;    self._warn_or_fail_if_strict(f&quot;Unknown config option: {key}\\n&quot;)&#x27;</span><br/><span class="error">E           and: &#x27;&#x27;</span><br/><span class="error">E           and: &#x27;-- Docs: https://docs.pytest.org/en/stable/warnings.html&#x27;</span><br/><span class="error">E           and: &#x27;=========================== short test summary info ===========================&#x27;</span><br/><span class="error">E           and: &#x27;ERROR test_hello_ini_setting.py::test_hello_world - ValueError: unknown confi...&#x27;</span><br/><span class="error">E           and: &#x27;======================== 2 warnings, 1 error in 0.09s =========================&#x27;</span><br/><span class="error">E       remains unmatched: &#x27;*::test_hello_world PASSED*&#x27;</span><br/><br/>C:\workspace\pytest-smtp\tests\test_smtp.py:59: Failed<br/> ------------------------------Captured stdout call------------------------------ <br/>============================= test session starts =============================
platform win32 -- Python 3.9.0, pytest-6.2.2, py-1.10.0, pluggy-0.13.1 -- C:\Users\Admin\.virtualenvs\pytest-smtp\Scripts\python.exe
cachedir: .pytest_cache
metadata: {&#x27;Python&#x27;: &#x27;3.9.0&#x27;, &#x27;Platform&#x27;: &#x27;Windows-10-10.0.19041-SP0&#x27;, &#x27;Packages&#x27;: {&#x27;pytest&#x27;: &#x27;6.2.2&#x27;, &#x27;py&#x27;: &#x27;1.10.0&#x27;, &#x27;pluggy&#x27;: &#x27;0.13.1&#x27;}, &#x27;Plugins&#x27;: {&#x27;html&#x27;: &#x27;3.1.1&#x27;, &#x27;metadata&#x27;: &#x27;1.11.0&#x27;, &#x27;repeat&#x27;: &#x27;0.9.1&#x27;, &#x27;smtp&#x27;: &#x27;0.1&#x27;}}
rootdir: C:\Users\Admin\AppData\Local\Temp\pytest-of-Admin\pytest-6\test_hello_ini_setting0, configfile: tox.ini
plugins: html-3.1.1, metadata-1.11.0, repeat-0.9.1, smtp-0.1
collecting ... collected 1 item

test_hello_ini_setting.py::test_hello_world ERROR                        [100%]

=================================== ERRORS ====================================
_____________________ ERROR at setup of test_hello_world ______________________

self = &lt;_pytest.config.Config object at 0x000001F01EA416A0&gt;, name = &#x27;HELLO&#x27;

    def getini(self, name: str):
        &quot;&quot;&quot;Return configuration value from an :ref:`ini file &lt;configfiles&gt;`.
    
        If the specified name hasn&#x27;t been registered through a prior
        :py:func:`parser.addini &lt;_pytest.config.argparsing.Parser.addini&gt;`
        call (usually from a plugin), a ValueError is raised.
        &quot;&quot;&quot;
        try:
&gt;           return self._inicache[name]
E           KeyError: &#x27;HELLO&#x27;

C:\Users\Admin\.virtualenvs\pytest-smtp\lib\site-packages\_pytest\config\__init__.py:1359: KeyError

During handling of the above exception, another exception occurred:

self = &lt;_pytest.config.Config object at 0x000001F01EA416A0&gt;, name = &#x27;HELLO&#x27;

    def _getini(self, name: str):
        try:
&gt;           description, type, default = self._parser._inidict[name]
E           KeyError: &#x27;HELLO&#x27;

C:\Users\Admin\.virtualenvs\pytest-smtp\lib\site-packages\_pytest\config\__init__.py:1366: KeyError

The above exception was the direct cause of the following exception:

request = &lt;SubRequest &#x27;hello&#x27; for &lt;Function test_hello_world&gt;&gt;

    @pytest.fixture
    def hello(request):
&gt;       return request.config.getini(&#x27;HELLO&#x27;)

test_hello_ini_setting.py:5: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
C:\Users\Admin\.virtualenvs\pytest-smtp\lib\site-packages\_pytest\config\__init__.py:1361: in getini
    self._inicache[name] = val = self._getini(name)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = &lt;_pytest.config.Config object at 0x000001F01EA416A0&gt;, name = &#x27;HELLO&#x27;

    def _getini(self, name: str):
        try:
            description, type, default = self._parser._inidict[name]
        except KeyError as e:
&gt;           raise ValueError(f&quot;unknown configuration value: {name!r}&quot;) from e
E           ValueError: unknown configuration value: &#x27;HELLO&#x27;

C:\Users\Admin\.virtualenvs\pytest-smtp\lib\site-packages\_pytest\config\__init__.py:1368: ValueError
============================== warnings summary ===============================
..\..\..\..\..\..\.virtualenvs\pytest-smtp\lib\site-packages\_pytest\config\__init__.py:1114
  C:\Users\Admin\.virtualenvs\pytest-smtp\lib\site-packages\_pytest\config\__init__.py:1114: PytestAssertRewriteWarning: Module already imported so cannot be rewritten: pytest_smtp
    self._mark_plugins_for_rewrite(hook)

..\..\..\..\..\..\.virtualenvs\pytest-smtp\lib\site-packages\_pytest\config\__init__.py:1233
  C:\Users\Admin\.virtualenvs\pytest-smtp\lib\site-packages\_pytest\config\__init__.py:1233: PytestConfigWarning: Unknown config option: HELLO
  
    self._warn_or_fail_if_strict(f&quot;Unknown config option: {key}\n&quot;)

-- Docs: https://docs.pytest.org/en/stable/warnings.html
=========================== short test summary info ===========================
ERROR test_hello_ini_setting.py::test_hello_world - ValueError: unknown confi...
======================== 2 warnings, 1 error in 0.09s =========================
<br/></div></td></tr></tbody></table></body></html>